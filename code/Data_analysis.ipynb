{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a966c8cb",
   "metadata": {},
   "source": [
    "# Data analysis for MSc5_research_project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0caba",
   "metadata": {},
   "source": [
    "This jupyter notebook deals with analysing the data for my research project within the MSc05 course in the Neurocognitive Psychology lab at Goethe University Frankfurt within the psychology master degree program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6922c12",
   "metadata": {},
   "source": [
    "Just to repeat briefly, the aim of the project is to use machine learning in order to predict whether a particpant can be classified either as control or patient with psychotic disorder based on different brain modalities. Before starting with our first modality being **cortical thickness (CT)**, the learning problem and the task type should be defined to know which model suits the best for the purpose of the project aim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11812b4b",
   "metadata": {},
   "source": [
    "## 1. Learning problem and task type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eea560",
   "metadata": {},
   "source": [
    "Considering the task type, our purpose is to classify the samples in two categories being control and patient. Hence, the task type is **classification**. For that, I want to use the given information regarding the labels for each sample, consequently the learning problem is **supervised**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd2f62",
   "metadata": {},
   "source": [
    "More specifically, with the data at hand the task type is **binary classification**. **Binary classification** refers to those classification task that have two class labels (in this data set control/patient). \n",
    "Commonly used algorithms for binary classification include **Logistic Regression, k-Nearest Neighbors, Decision Trees, Support Vector Machine and Naive Bayes** (for further information click [here](https://machinelearningmastery.com/types-of-classification-in-machine-learning/)).\n",
    "In this project, I will focus on two algorithms being **Logistic Regression** and **Support Vector Machine**. For the latter, there is a separate notebook called **SVM**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0d430",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150941c9",
   "metadata": {},
   "source": [
    "### 2.1 Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e032f4",
   "metadata": {},
   "source": [
    "First of all, I use CT data as my input data for the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c372d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c724d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "\n",
    "CT_Dublin_path = os.path.join(os.pardir, 'data', 'PARC_500.aparc_thickness_Dublin.csv')\n",
    "CT_Dublin = pd.read_csv(CT_Dublin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab3630c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Group</th>\n",
       "      <th>lh_bankssts_part1_thickness</th>\n",
       "      <th>lh_bankssts_part2_thickness</th>\n",
       "      <th>lh_caudalanteriorcingulate_part1_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part1_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part2_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part3_thickness</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_part5_thickness</th>\n",
       "      <th>rh_supramarginal_part6_thickness</th>\n",
       "      <th>rh_supramarginal_part7_thickness</th>\n",
       "      <th>rh_frontalpole_part1_thickness</th>\n",
       "      <th>rh_temporalpole_part1_thickness</th>\n",
       "      <th>rh_transversetemporal_part1_thickness</th>\n",
       "      <th>rh_insula_part1_thickness</th>\n",
       "      <th>rh_insula_part2_thickness</th>\n",
       "      <th>rh_insula_part3_thickness</th>\n",
       "      <th>rh_insula_part4_thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CON9225</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.180</td>\n",
       "      <td>2.382</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.526</td>\n",
       "      <td>2.747</td>\n",
       "      <td>2.544</td>\n",
       "      <td>...</td>\n",
       "      <td>2.817</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.430</td>\n",
       "      <td>3.004</td>\n",
       "      <td>3.979</td>\n",
       "      <td>2.329</td>\n",
       "      <td>3.620</td>\n",
       "      <td>2.776</td>\n",
       "      <td>3.282</td>\n",
       "      <td>3.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CON9229</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.394</td>\n",
       "      <td>1.973</td>\n",
       "      <td>2.534</td>\n",
       "      <td>2.439</td>\n",
       "      <td>2.485</td>\n",
       "      <td>2.435</td>\n",
       "      <td>...</td>\n",
       "      <td>2.611</td>\n",
       "      <td>2.418</td>\n",
       "      <td>2.317</td>\n",
       "      <td>2.794</td>\n",
       "      <td>3.851</td>\n",
       "      <td>2.034</td>\n",
       "      <td>3.588</td>\n",
       "      <td>2.654</td>\n",
       "      <td>3.124</td>\n",
       "      <td>3.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CON9231</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.567</td>\n",
       "      <td>1.954</td>\n",
       "      <td>2.439</td>\n",
       "      <td>2.428</td>\n",
       "      <td>2.190</td>\n",
       "      <td>...</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.309</td>\n",
       "      <td>2.390</td>\n",
       "      <td>2.365</td>\n",
       "      <td>4.039</td>\n",
       "      <td>2.337</td>\n",
       "      <td>3.657</td>\n",
       "      <td>2.495</td>\n",
       "      <td>2.669</td>\n",
       "      <td>2.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GASP3037</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.187</td>\n",
       "      <td>1.923</td>\n",
       "      <td>2.160</td>\n",
       "      <td>2.410</td>\n",
       "      <td>2.381</td>\n",
       "      <td>2.277</td>\n",
       "      <td>...</td>\n",
       "      <td>2.265</td>\n",
       "      <td>2.306</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.281</td>\n",
       "      <td>3.505</td>\n",
       "      <td>2.275</td>\n",
       "      <td>3.121</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.604</td>\n",
       "      <td>2.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GASP3040</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.862</td>\n",
       "      <td>1.750</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.516</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.169</td>\n",
       "      <td>...</td>\n",
       "      <td>2.582</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.047</td>\n",
       "      <td>2.389</td>\n",
       "      <td>3.272</td>\n",
       "      <td>2.445</td>\n",
       "      <td>3.171</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.659</td>\n",
       "      <td>2.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>RPG9019</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.150</td>\n",
       "      <td>1.995</td>\n",
       "      <td>2.254</td>\n",
       "      <td>2.164</td>\n",
       "      <td>2.008</td>\n",
       "      <td>...</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.288</td>\n",
       "      <td>2.395</td>\n",
       "      <td>2.105</td>\n",
       "      <td>3.267</td>\n",
       "      <td>2.257</td>\n",
       "      <td>3.231</td>\n",
       "      <td>2.574</td>\n",
       "      <td>2.920</td>\n",
       "      <td>2.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>RPG9102</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.124</td>\n",
       "      <td>2.531</td>\n",
       "      <td>2.502</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.183</td>\n",
       "      <td>...</td>\n",
       "      <td>2.302</td>\n",
       "      <td>2.182</td>\n",
       "      <td>2.182</td>\n",
       "      <td>2.327</td>\n",
       "      <td>2.881</td>\n",
       "      <td>2.124</td>\n",
       "      <td>3.159</td>\n",
       "      <td>2.450</td>\n",
       "      <td>2.753</td>\n",
       "      <td>2.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>RPG9119</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.559</td>\n",
       "      <td>2.578</td>\n",
       "      <td>2.463</td>\n",
       "      <td>2.463</td>\n",
       "      <td>2.053</td>\n",
       "      <td>...</td>\n",
       "      <td>2.534</td>\n",
       "      <td>2.604</td>\n",
       "      <td>2.449</td>\n",
       "      <td>2.370</td>\n",
       "      <td>3.111</td>\n",
       "      <td>2.190</td>\n",
       "      <td>3.480</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.571</td>\n",
       "      <td>2.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>RPG9121</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.940</td>\n",
       "      <td>2.438</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.610</td>\n",
       "      <td>2.099</td>\n",
       "      <td>...</td>\n",
       "      <td>2.638</td>\n",
       "      <td>2.225</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.115</td>\n",
       "      <td>3.853</td>\n",
       "      <td>2.231</td>\n",
       "      <td>3.187</td>\n",
       "      <td>2.510</td>\n",
       "      <td>2.759</td>\n",
       "      <td>2.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>RPG9126</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.108</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.145</td>\n",
       "      <td>2.192</td>\n",
       "      <td>2.443</td>\n",
       "      <td>1.977</td>\n",
       "      <td>...</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.251</td>\n",
       "      <td>2.021</td>\n",
       "      <td>2.419</td>\n",
       "      <td>3.679</td>\n",
       "      <td>1.970</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.855</td>\n",
       "      <td>2.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject ID  Age  Sex  Group  lh_bankssts_part1_thickness  \\\n",
       "0      CON9225   21    2      1                        2.180   \n",
       "1      CON9229   28    2      1                        2.394   \n",
       "2      CON9231   29    2      1                        2.551   \n",
       "3     GASP3037   61    1      2                        2.187   \n",
       "4     GASP3040   47    1      2                        1.862   \n",
       "..         ...  ...  ...    ...                          ...   \n",
       "103    RPG9019   31    1      2                        2.240   \n",
       "104    RPG9102   42    2      2                        2.269   \n",
       "105    RPG9119   41    1      2                        2.273   \n",
       "106    RPG9121   51    1      2                        1.940   \n",
       "107    RPG9126   56    1      2                        2.108   \n",
       "\n",
       "     lh_bankssts_part2_thickness  lh_caudalanteriorcingulate_part1_thickness  \\\n",
       "0                          2.382                                       2.346   \n",
       "1                          1.973                                       2.534   \n",
       "2                          2.567                                       1.954   \n",
       "3                          1.923                                       2.160   \n",
       "4                          1.750                                       2.129   \n",
       "..                           ...                                         ...   \n",
       "103                        2.150                                       1.995   \n",
       "104                        2.124                                       2.531   \n",
       "105                        2.559                                       2.578   \n",
       "106                        2.438                                       2.272   \n",
       "107                        2.269                                       2.145   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part1_thickness  \\\n",
       "0                                     2.526   \n",
       "1                                     2.439   \n",
       "2                                     2.439   \n",
       "3                                     2.410   \n",
       "4                                     2.516   \n",
       "..                                      ...   \n",
       "103                                   2.254   \n",
       "104                                   2.502   \n",
       "105                                   2.463   \n",
       "106                                   2.272   \n",
       "107                                   2.192   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part2_thickness  \\\n",
       "0                                     2.747   \n",
       "1                                     2.485   \n",
       "2                                     2.428   \n",
       "3                                     2.381   \n",
       "4                                     2.244   \n",
       "..                                      ...   \n",
       "103                                   2.164   \n",
       "104                                   2.250   \n",
       "105                                   2.463   \n",
       "106                                   2.610   \n",
       "107                                   2.443   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part3_thickness  ...  \\\n",
       "0                                     2.544  ...   \n",
       "1                                     2.435  ...   \n",
       "2                                     2.190  ...   \n",
       "3                                     2.277  ...   \n",
       "4                                     2.169  ...   \n",
       "..                                      ...  ...   \n",
       "103                                   2.008  ...   \n",
       "104                                   2.183  ...   \n",
       "105                                   2.053  ...   \n",
       "106                                   2.099  ...   \n",
       "107                                   1.977  ...   \n",
       "\n",
       "     rh_supramarginal_part5_thickness  rh_supramarginal_part6_thickness  \\\n",
       "0                               2.817                             2.325   \n",
       "1                               2.611                             2.418   \n",
       "2                               2.777                             2.309   \n",
       "3                               2.265                             2.306   \n",
       "4                               2.582                             2.314   \n",
       "..                                ...                               ...   \n",
       "103                             2.273                             2.288   \n",
       "104                             2.302                             2.182   \n",
       "105                             2.534                             2.604   \n",
       "106                             2.638                             2.225   \n",
       "107                             2.013                             2.251   \n",
       "\n",
       "     rh_supramarginal_part7_thickness  rh_frontalpole_part1_thickness  \\\n",
       "0                               2.430                           3.004   \n",
       "1                               2.317                           2.794   \n",
       "2                               2.390                           2.365   \n",
       "3                               2.129                           2.281   \n",
       "4                               2.047                           2.389   \n",
       "..                                ...                             ...   \n",
       "103                             2.395                           2.105   \n",
       "104                             2.182                           2.327   \n",
       "105                             2.449                           2.370   \n",
       "106                             2.013                           2.115   \n",
       "107                             2.021                           2.419   \n",
       "\n",
       "     rh_temporalpole_part1_thickness  rh_transversetemporal_part1_thickness  \\\n",
       "0                              3.979                                  2.329   \n",
       "1                              3.851                                  2.034   \n",
       "2                              4.039                                  2.337   \n",
       "3                              3.505                                  2.275   \n",
       "4                              3.272                                  2.445   \n",
       "..                               ...                                    ...   \n",
       "103                            3.267                                  2.257   \n",
       "104                            2.881                                  2.124   \n",
       "105                            3.111                                  2.190   \n",
       "106                            3.853                                  2.231   \n",
       "107                            3.679                                  1.970   \n",
       "\n",
       "     rh_insula_part1_thickness  rh_insula_part2_thickness  \\\n",
       "0                        3.620                      2.776   \n",
       "1                        3.588                      2.654   \n",
       "2                        3.657                      2.495   \n",
       "3                        3.121                      2.333   \n",
       "4                        3.171                      2.216   \n",
       "..                         ...                        ...   \n",
       "103                      3.231                      2.574   \n",
       "104                      3.159                      2.450   \n",
       "105                      3.480                      2.294   \n",
       "106                      3.187                      2.510   \n",
       "107                      3.192                      2.551   \n",
       "\n",
       "     rh_insula_part3_thickness  rh_insula_part4_thickness  \n",
       "0                        3.282                      3.347  \n",
       "1                        3.124                      3.214  \n",
       "2                        2.669                      2.886  \n",
       "3                        2.604                      2.731  \n",
       "4                        2.659                      2.657  \n",
       "..                         ...                        ...  \n",
       "103                      2.920                      2.899  \n",
       "104                      2.753                      2.791  \n",
       "105                      2.571                      2.875  \n",
       "106                      2.759                      2.838  \n",
       "107                      2.855                      2.985  \n",
       "\n",
       "[108 rows x 312 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CT_Dublin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc679ec0",
   "metadata": {},
   "source": [
    "The data contains variables such as SubjectID, Age and Sex which are not relevant for the classification. Hence, we adjust the dataframe accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7eaad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust dataframe\n",
    "\n",
    "CT_Dublin_adj = CT_Dublin.drop(['Subject ID','Age', 'Sex'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1277a861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>lh_bankssts_part1_thickness</th>\n",
       "      <th>lh_bankssts_part2_thickness</th>\n",
       "      <th>lh_caudalanteriorcingulate_part1_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part1_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part2_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part3_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part4_thickness</th>\n",
       "      <th>lh_cuneus_part1_thickness</th>\n",
       "      <th>lh_cuneus_part2_thickness</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_part5_thickness</th>\n",
       "      <th>rh_supramarginal_part6_thickness</th>\n",
       "      <th>rh_supramarginal_part7_thickness</th>\n",
       "      <th>rh_frontalpole_part1_thickness</th>\n",
       "      <th>rh_temporalpole_part1_thickness</th>\n",
       "      <th>rh_transversetemporal_part1_thickness</th>\n",
       "      <th>rh_insula_part1_thickness</th>\n",
       "      <th>rh_insula_part2_thickness</th>\n",
       "      <th>rh_insula_part3_thickness</th>\n",
       "      <th>rh_insula_part4_thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.180</td>\n",
       "      <td>2.382</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.526</td>\n",
       "      <td>2.747</td>\n",
       "      <td>2.544</td>\n",
       "      <td>2.582</td>\n",
       "      <td>1.816</td>\n",
       "      <td>2.228</td>\n",
       "      <td>...</td>\n",
       "      <td>2.817</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.430</td>\n",
       "      <td>3.004</td>\n",
       "      <td>3.979</td>\n",
       "      <td>2.329</td>\n",
       "      <td>3.620</td>\n",
       "      <td>2.776</td>\n",
       "      <td>3.282</td>\n",
       "      <td>3.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.394</td>\n",
       "      <td>1.973</td>\n",
       "      <td>2.534</td>\n",
       "      <td>2.439</td>\n",
       "      <td>2.485</td>\n",
       "      <td>2.435</td>\n",
       "      <td>2.458</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.821</td>\n",
       "      <td>...</td>\n",
       "      <td>2.611</td>\n",
       "      <td>2.418</td>\n",
       "      <td>2.317</td>\n",
       "      <td>2.794</td>\n",
       "      <td>3.851</td>\n",
       "      <td>2.034</td>\n",
       "      <td>3.588</td>\n",
       "      <td>2.654</td>\n",
       "      <td>3.124</td>\n",
       "      <td>3.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.567</td>\n",
       "      <td>1.954</td>\n",
       "      <td>2.439</td>\n",
       "      <td>2.428</td>\n",
       "      <td>2.190</td>\n",
       "      <td>2.377</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.800</td>\n",
       "      <td>...</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.309</td>\n",
       "      <td>2.390</td>\n",
       "      <td>2.365</td>\n",
       "      <td>4.039</td>\n",
       "      <td>2.337</td>\n",
       "      <td>3.657</td>\n",
       "      <td>2.495</td>\n",
       "      <td>2.669</td>\n",
       "      <td>2.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2.187</td>\n",
       "      <td>1.923</td>\n",
       "      <td>2.160</td>\n",
       "      <td>2.410</td>\n",
       "      <td>2.381</td>\n",
       "      <td>2.277</td>\n",
       "      <td>2.361</td>\n",
       "      <td>1.585</td>\n",
       "      <td>1.750</td>\n",
       "      <td>...</td>\n",
       "      <td>2.265</td>\n",
       "      <td>2.306</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.281</td>\n",
       "      <td>3.505</td>\n",
       "      <td>2.275</td>\n",
       "      <td>3.121</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.604</td>\n",
       "      <td>2.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.862</td>\n",
       "      <td>1.750</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.516</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.169</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.646</td>\n",
       "      <td>1.717</td>\n",
       "      <td>...</td>\n",
       "      <td>2.582</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.047</td>\n",
       "      <td>2.389</td>\n",
       "      <td>3.272</td>\n",
       "      <td>2.445</td>\n",
       "      <td>3.171</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.659</td>\n",
       "      <td>2.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.150</td>\n",
       "      <td>1.995</td>\n",
       "      <td>2.254</td>\n",
       "      <td>2.164</td>\n",
       "      <td>2.008</td>\n",
       "      <td>2.298</td>\n",
       "      <td>1.918</td>\n",
       "      <td>1.717</td>\n",
       "      <td>...</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.288</td>\n",
       "      <td>2.395</td>\n",
       "      <td>2.105</td>\n",
       "      <td>3.267</td>\n",
       "      <td>2.257</td>\n",
       "      <td>3.231</td>\n",
       "      <td>2.574</td>\n",
       "      <td>2.920</td>\n",
       "      <td>2.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.124</td>\n",
       "      <td>2.531</td>\n",
       "      <td>2.502</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.183</td>\n",
       "      <td>2.408</td>\n",
       "      <td>1.539</td>\n",
       "      <td>1.611</td>\n",
       "      <td>...</td>\n",
       "      <td>2.302</td>\n",
       "      <td>2.182</td>\n",
       "      <td>2.182</td>\n",
       "      <td>2.327</td>\n",
       "      <td>2.881</td>\n",
       "      <td>2.124</td>\n",
       "      <td>3.159</td>\n",
       "      <td>2.450</td>\n",
       "      <td>2.753</td>\n",
       "      <td>2.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.559</td>\n",
       "      <td>2.578</td>\n",
       "      <td>2.463</td>\n",
       "      <td>2.463</td>\n",
       "      <td>2.053</td>\n",
       "      <td>2.526</td>\n",
       "      <td>1.733</td>\n",
       "      <td>1.859</td>\n",
       "      <td>...</td>\n",
       "      <td>2.534</td>\n",
       "      <td>2.604</td>\n",
       "      <td>2.449</td>\n",
       "      <td>2.370</td>\n",
       "      <td>3.111</td>\n",
       "      <td>2.190</td>\n",
       "      <td>3.480</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.571</td>\n",
       "      <td>2.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2</td>\n",
       "      <td>1.940</td>\n",
       "      <td>2.438</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.610</td>\n",
       "      <td>2.099</td>\n",
       "      <td>2.538</td>\n",
       "      <td>1.931</td>\n",
       "      <td>1.792</td>\n",
       "      <td>...</td>\n",
       "      <td>2.638</td>\n",
       "      <td>2.225</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.115</td>\n",
       "      <td>3.853</td>\n",
       "      <td>2.231</td>\n",
       "      <td>3.187</td>\n",
       "      <td>2.510</td>\n",
       "      <td>2.759</td>\n",
       "      <td>2.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2</td>\n",
       "      <td>2.108</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.145</td>\n",
       "      <td>2.192</td>\n",
       "      <td>2.443</td>\n",
       "      <td>1.977</td>\n",
       "      <td>2.453</td>\n",
       "      <td>1.590</td>\n",
       "      <td>1.715</td>\n",
       "      <td>...</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.251</td>\n",
       "      <td>2.021</td>\n",
       "      <td>2.419</td>\n",
       "      <td>3.679</td>\n",
       "      <td>1.970</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.855</td>\n",
       "      <td>2.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  lh_bankssts_part1_thickness  lh_bankssts_part2_thickness  \\\n",
       "0        1                        2.180                        2.382   \n",
       "1        1                        2.394                        1.973   \n",
       "2        1                        2.551                        2.567   \n",
       "3        2                        2.187                        1.923   \n",
       "4        2                        1.862                        1.750   \n",
       "..     ...                          ...                          ...   \n",
       "103      2                        2.240                        2.150   \n",
       "104      2                        2.269                        2.124   \n",
       "105      2                        2.273                        2.559   \n",
       "106      2                        1.940                        2.438   \n",
       "107      2                        2.108                        2.269   \n",
       "\n",
       "     lh_caudalanteriorcingulate_part1_thickness  \\\n",
       "0                                         2.346   \n",
       "1                                         2.534   \n",
       "2                                         1.954   \n",
       "3                                         2.160   \n",
       "4                                         2.129   \n",
       "..                                          ...   \n",
       "103                                       1.995   \n",
       "104                                       2.531   \n",
       "105                                       2.578   \n",
       "106                                       2.272   \n",
       "107                                       2.145   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part1_thickness  \\\n",
       "0                                     2.526   \n",
       "1                                     2.439   \n",
       "2                                     2.439   \n",
       "3                                     2.410   \n",
       "4                                     2.516   \n",
       "..                                      ...   \n",
       "103                                   2.254   \n",
       "104                                   2.502   \n",
       "105                                   2.463   \n",
       "106                                   2.272   \n",
       "107                                   2.192   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part2_thickness  \\\n",
       "0                                     2.747   \n",
       "1                                     2.485   \n",
       "2                                     2.428   \n",
       "3                                     2.381   \n",
       "4                                     2.244   \n",
       "..                                      ...   \n",
       "103                                   2.164   \n",
       "104                                   2.250   \n",
       "105                                   2.463   \n",
       "106                                   2.610   \n",
       "107                                   2.443   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part3_thickness  \\\n",
       "0                                     2.544   \n",
       "1                                     2.435   \n",
       "2                                     2.190   \n",
       "3                                     2.277   \n",
       "4                                     2.169   \n",
       "..                                      ...   \n",
       "103                                   2.008   \n",
       "104                                   2.183   \n",
       "105                                   2.053   \n",
       "106                                   2.099   \n",
       "107                                   1.977   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part4_thickness  lh_cuneus_part1_thickness  \\\n",
       "0                                     2.582                      1.816   \n",
       "1                                     2.458                      1.723   \n",
       "2                                     2.377                      2.026   \n",
       "3                                     2.361                      1.585   \n",
       "4                                     2.220                      1.646   \n",
       "..                                      ...                        ...   \n",
       "103                                   2.298                      1.918   \n",
       "104                                   2.408                      1.539   \n",
       "105                                   2.526                      1.733   \n",
       "106                                   2.538                      1.931   \n",
       "107                                   2.453                      1.590   \n",
       "\n",
       "     lh_cuneus_part2_thickness  ...  rh_supramarginal_part5_thickness  \\\n",
       "0                        2.228  ...                             2.817   \n",
       "1                        1.821  ...                             2.611   \n",
       "2                        1.800  ...                             2.777   \n",
       "3                        1.750  ...                             2.265   \n",
       "4                        1.717  ...                             2.582   \n",
       "..                         ...  ...                               ...   \n",
       "103                      1.717  ...                             2.273   \n",
       "104                      1.611  ...                             2.302   \n",
       "105                      1.859  ...                             2.534   \n",
       "106                      1.792  ...                             2.638   \n",
       "107                      1.715  ...                             2.013   \n",
       "\n",
       "     rh_supramarginal_part6_thickness  rh_supramarginal_part7_thickness  \\\n",
       "0                               2.325                             2.430   \n",
       "1                               2.418                             2.317   \n",
       "2                               2.309                             2.390   \n",
       "3                               2.306                             2.129   \n",
       "4                               2.314                             2.047   \n",
       "..                                ...                               ...   \n",
       "103                             2.288                             2.395   \n",
       "104                             2.182                             2.182   \n",
       "105                             2.604                             2.449   \n",
       "106                             2.225                             2.013   \n",
       "107                             2.251                             2.021   \n",
       "\n",
       "     rh_frontalpole_part1_thickness  rh_temporalpole_part1_thickness  \\\n",
       "0                             3.004                            3.979   \n",
       "1                             2.794                            3.851   \n",
       "2                             2.365                            4.039   \n",
       "3                             2.281                            3.505   \n",
       "4                             2.389                            3.272   \n",
       "..                              ...                              ...   \n",
       "103                           2.105                            3.267   \n",
       "104                           2.327                            2.881   \n",
       "105                           2.370                            3.111   \n",
       "106                           2.115                            3.853   \n",
       "107                           2.419                            3.679   \n",
       "\n",
       "     rh_transversetemporal_part1_thickness  rh_insula_part1_thickness  \\\n",
       "0                                    2.329                      3.620   \n",
       "1                                    2.034                      3.588   \n",
       "2                                    2.337                      3.657   \n",
       "3                                    2.275                      3.121   \n",
       "4                                    2.445                      3.171   \n",
       "..                                     ...                        ...   \n",
       "103                                  2.257                      3.231   \n",
       "104                                  2.124                      3.159   \n",
       "105                                  2.190                      3.480   \n",
       "106                                  2.231                      3.187   \n",
       "107                                  1.970                      3.192   \n",
       "\n",
       "     rh_insula_part2_thickness  rh_insula_part3_thickness  \\\n",
       "0                        2.776                      3.282   \n",
       "1                        2.654                      3.124   \n",
       "2                        2.495                      2.669   \n",
       "3                        2.333                      2.604   \n",
       "4                        2.216                      2.659   \n",
       "..                         ...                        ...   \n",
       "103                      2.574                      2.920   \n",
       "104                      2.450                      2.753   \n",
       "105                      2.294                      2.571   \n",
       "106                      2.510                      2.759   \n",
       "107                      2.551                      2.855   \n",
       "\n",
       "     rh_insula_part4_thickness  \n",
       "0                        3.347  \n",
       "1                        3.214  \n",
       "2                        2.886  \n",
       "3                        2.731  \n",
       "4                        2.657  \n",
       "..                         ...  \n",
       "103                      2.899  \n",
       "104                      2.791  \n",
       "105                      2.875  \n",
       "106                      2.838  \n",
       "107                      2.985  \n",
       "\n",
       "[108 rows x 309 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CT_Dublin_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074688be",
   "metadata": {},
   "source": [
    "As the dataframe shows, the Group variable contains information of whether the samples belong to control or patient. In this case, 1 indicates control and 2 patient. In order to perform a **Logistic Regression**, the labels of the outputs requires to be 0 and 1 since the probability of an instance belonging to a default class is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca4d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label group 1 as 0 and 2 as 1\n",
    "\n",
    "CT_Dublin_adj['Group'] = CT_Dublin_adj['Group'].replace([1,2],[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408610ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>lh_bankssts_part1_thickness</th>\n",
       "      <th>lh_bankssts_part2_thickness</th>\n",
       "      <th>lh_caudalanteriorcingulate_part1_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part1_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part2_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part3_thickness</th>\n",
       "      <th>lh_caudalmiddlefrontal_part4_thickness</th>\n",
       "      <th>lh_cuneus_part1_thickness</th>\n",
       "      <th>lh_cuneus_part2_thickness</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_part5_thickness</th>\n",
       "      <th>rh_supramarginal_part6_thickness</th>\n",
       "      <th>rh_supramarginal_part7_thickness</th>\n",
       "      <th>rh_frontalpole_part1_thickness</th>\n",
       "      <th>rh_temporalpole_part1_thickness</th>\n",
       "      <th>rh_transversetemporal_part1_thickness</th>\n",
       "      <th>rh_insula_part1_thickness</th>\n",
       "      <th>rh_insula_part2_thickness</th>\n",
       "      <th>rh_insula_part3_thickness</th>\n",
       "      <th>rh_insula_part4_thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.180</td>\n",
       "      <td>2.382</td>\n",
       "      <td>2.346</td>\n",
       "      <td>2.526</td>\n",
       "      <td>2.747</td>\n",
       "      <td>2.544</td>\n",
       "      <td>2.582</td>\n",
       "      <td>1.816</td>\n",
       "      <td>2.228</td>\n",
       "      <td>...</td>\n",
       "      <td>2.817</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.430</td>\n",
       "      <td>3.004</td>\n",
       "      <td>3.979</td>\n",
       "      <td>2.329</td>\n",
       "      <td>3.620</td>\n",
       "      <td>2.776</td>\n",
       "      <td>3.282</td>\n",
       "      <td>3.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.394</td>\n",
       "      <td>1.973</td>\n",
       "      <td>2.534</td>\n",
       "      <td>2.439</td>\n",
       "      <td>2.485</td>\n",
       "      <td>2.435</td>\n",
       "      <td>2.458</td>\n",
       "      <td>1.723</td>\n",
       "      <td>1.821</td>\n",
       "      <td>...</td>\n",
       "      <td>2.611</td>\n",
       "      <td>2.418</td>\n",
       "      <td>2.317</td>\n",
       "      <td>2.794</td>\n",
       "      <td>3.851</td>\n",
       "      <td>2.034</td>\n",
       "      <td>3.588</td>\n",
       "      <td>2.654</td>\n",
       "      <td>3.124</td>\n",
       "      <td>3.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.567</td>\n",
       "      <td>1.954</td>\n",
       "      <td>2.439</td>\n",
       "      <td>2.428</td>\n",
       "      <td>2.190</td>\n",
       "      <td>2.377</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.800</td>\n",
       "      <td>...</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.309</td>\n",
       "      <td>2.390</td>\n",
       "      <td>2.365</td>\n",
       "      <td>4.039</td>\n",
       "      <td>2.337</td>\n",
       "      <td>3.657</td>\n",
       "      <td>2.495</td>\n",
       "      <td>2.669</td>\n",
       "      <td>2.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.187</td>\n",
       "      <td>1.923</td>\n",
       "      <td>2.160</td>\n",
       "      <td>2.410</td>\n",
       "      <td>2.381</td>\n",
       "      <td>2.277</td>\n",
       "      <td>2.361</td>\n",
       "      <td>1.585</td>\n",
       "      <td>1.750</td>\n",
       "      <td>...</td>\n",
       "      <td>2.265</td>\n",
       "      <td>2.306</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.281</td>\n",
       "      <td>3.505</td>\n",
       "      <td>2.275</td>\n",
       "      <td>3.121</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.604</td>\n",
       "      <td>2.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.862</td>\n",
       "      <td>1.750</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.516</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.169</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.646</td>\n",
       "      <td>1.717</td>\n",
       "      <td>...</td>\n",
       "      <td>2.582</td>\n",
       "      <td>2.314</td>\n",
       "      <td>2.047</td>\n",
       "      <td>2.389</td>\n",
       "      <td>3.272</td>\n",
       "      <td>2.445</td>\n",
       "      <td>3.171</td>\n",
       "      <td>2.216</td>\n",
       "      <td>2.659</td>\n",
       "      <td>2.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.150</td>\n",
       "      <td>1.995</td>\n",
       "      <td>2.254</td>\n",
       "      <td>2.164</td>\n",
       "      <td>2.008</td>\n",
       "      <td>2.298</td>\n",
       "      <td>1.918</td>\n",
       "      <td>1.717</td>\n",
       "      <td>...</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.288</td>\n",
       "      <td>2.395</td>\n",
       "      <td>2.105</td>\n",
       "      <td>3.267</td>\n",
       "      <td>2.257</td>\n",
       "      <td>3.231</td>\n",
       "      <td>2.574</td>\n",
       "      <td>2.920</td>\n",
       "      <td>2.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.124</td>\n",
       "      <td>2.531</td>\n",
       "      <td>2.502</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.183</td>\n",
       "      <td>2.408</td>\n",
       "      <td>1.539</td>\n",
       "      <td>1.611</td>\n",
       "      <td>...</td>\n",
       "      <td>2.302</td>\n",
       "      <td>2.182</td>\n",
       "      <td>2.182</td>\n",
       "      <td>2.327</td>\n",
       "      <td>2.881</td>\n",
       "      <td>2.124</td>\n",
       "      <td>3.159</td>\n",
       "      <td>2.450</td>\n",
       "      <td>2.753</td>\n",
       "      <td>2.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.559</td>\n",
       "      <td>2.578</td>\n",
       "      <td>2.463</td>\n",
       "      <td>2.463</td>\n",
       "      <td>2.053</td>\n",
       "      <td>2.526</td>\n",
       "      <td>1.733</td>\n",
       "      <td>1.859</td>\n",
       "      <td>...</td>\n",
       "      <td>2.534</td>\n",
       "      <td>2.604</td>\n",
       "      <td>2.449</td>\n",
       "      <td>2.370</td>\n",
       "      <td>3.111</td>\n",
       "      <td>2.190</td>\n",
       "      <td>3.480</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.571</td>\n",
       "      <td>2.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "      <td>1.940</td>\n",
       "      <td>2.438</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.272</td>\n",
       "      <td>2.610</td>\n",
       "      <td>2.099</td>\n",
       "      <td>2.538</td>\n",
       "      <td>1.931</td>\n",
       "      <td>1.792</td>\n",
       "      <td>...</td>\n",
       "      <td>2.638</td>\n",
       "      <td>2.225</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.115</td>\n",
       "      <td>3.853</td>\n",
       "      <td>2.231</td>\n",
       "      <td>3.187</td>\n",
       "      <td>2.510</td>\n",
       "      <td>2.759</td>\n",
       "      <td>2.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1</td>\n",
       "      <td>2.108</td>\n",
       "      <td>2.269</td>\n",
       "      <td>2.145</td>\n",
       "      <td>2.192</td>\n",
       "      <td>2.443</td>\n",
       "      <td>1.977</td>\n",
       "      <td>2.453</td>\n",
       "      <td>1.590</td>\n",
       "      <td>1.715</td>\n",
       "      <td>...</td>\n",
       "      <td>2.013</td>\n",
       "      <td>2.251</td>\n",
       "      <td>2.021</td>\n",
       "      <td>2.419</td>\n",
       "      <td>3.679</td>\n",
       "      <td>1.970</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.855</td>\n",
       "      <td>2.985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 309 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  lh_bankssts_part1_thickness  lh_bankssts_part2_thickness  \\\n",
       "0        0                        2.180                        2.382   \n",
       "1        0                        2.394                        1.973   \n",
       "2        0                        2.551                        2.567   \n",
       "3        1                        2.187                        1.923   \n",
       "4        1                        1.862                        1.750   \n",
       "..     ...                          ...                          ...   \n",
       "103      1                        2.240                        2.150   \n",
       "104      1                        2.269                        2.124   \n",
       "105      1                        2.273                        2.559   \n",
       "106      1                        1.940                        2.438   \n",
       "107      1                        2.108                        2.269   \n",
       "\n",
       "     lh_caudalanteriorcingulate_part1_thickness  \\\n",
       "0                                         2.346   \n",
       "1                                         2.534   \n",
       "2                                         1.954   \n",
       "3                                         2.160   \n",
       "4                                         2.129   \n",
       "..                                          ...   \n",
       "103                                       1.995   \n",
       "104                                       2.531   \n",
       "105                                       2.578   \n",
       "106                                       2.272   \n",
       "107                                       2.145   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part1_thickness  \\\n",
       "0                                     2.526   \n",
       "1                                     2.439   \n",
       "2                                     2.439   \n",
       "3                                     2.410   \n",
       "4                                     2.516   \n",
       "..                                      ...   \n",
       "103                                   2.254   \n",
       "104                                   2.502   \n",
       "105                                   2.463   \n",
       "106                                   2.272   \n",
       "107                                   2.192   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part2_thickness  \\\n",
       "0                                     2.747   \n",
       "1                                     2.485   \n",
       "2                                     2.428   \n",
       "3                                     2.381   \n",
       "4                                     2.244   \n",
       "..                                      ...   \n",
       "103                                   2.164   \n",
       "104                                   2.250   \n",
       "105                                   2.463   \n",
       "106                                   2.610   \n",
       "107                                   2.443   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part3_thickness  \\\n",
       "0                                     2.544   \n",
       "1                                     2.435   \n",
       "2                                     2.190   \n",
       "3                                     2.277   \n",
       "4                                     2.169   \n",
       "..                                      ...   \n",
       "103                                   2.008   \n",
       "104                                   2.183   \n",
       "105                                   2.053   \n",
       "106                                   2.099   \n",
       "107                                   1.977   \n",
       "\n",
       "     lh_caudalmiddlefrontal_part4_thickness  lh_cuneus_part1_thickness  \\\n",
       "0                                     2.582                      1.816   \n",
       "1                                     2.458                      1.723   \n",
       "2                                     2.377                      2.026   \n",
       "3                                     2.361                      1.585   \n",
       "4                                     2.220                      1.646   \n",
       "..                                      ...                        ...   \n",
       "103                                   2.298                      1.918   \n",
       "104                                   2.408                      1.539   \n",
       "105                                   2.526                      1.733   \n",
       "106                                   2.538                      1.931   \n",
       "107                                   2.453                      1.590   \n",
       "\n",
       "     lh_cuneus_part2_thickness  ...  rh_supramarginal_part5_thickness  \\\n",
       "0                        2.228  ...                             2.817   \n",
       "1                        1.821  ...                             2.611   \n",
       "2                        1.800  ...                             2.777   \n",
       "3                        1.750  ...                             2.265   \n",
       "4                        1.717  ...                             2.582   \n",
       "..                         ...  ...                               ...   \n",
       "103                      1.717  ...                             2.273   \n",
       "104                      1.611  ...                             2.302   \n",
       "105                      1.859  ...                             2.534   \n",
       "106                      1.792  ...                             2.638   \n",
       "107                      1.715  ...                             2.013   \n",
       "\n",
       "     rh_supramarginal_part6_thickness  rh_supramarginal_part7_thickness  \\\n",
       "0                               2.325                             2.430   \n",
       "1                               2.418                             2.317   \n",
       "2                               2.309                             2.390   \n",
       "3                               2.306                             2.129   \n",
       "4                               2.314                             2.047   \n",
       "..                                ...                               ...   \n",
       "103                             2.288                             2.395   \n",
       "104                             2.182                             2.182   \n",
       "105                             2.604                             2.449   \n",
       "106                             2.225                             2.013   \n",
       "107                             2.251                             2.021   \n",
       "\n",
       "     rh_frontalpole_part1_thickness  rh_temporalpole_part1_thickness  \\\n",
       "0                             3.004                            3.979   \n",
       "1                             2.794                            3.851   \n",
       "2                             2.365                            4.039   \n",
       "3                             2.281                            3.505   \n",
       "4                             2.389                            3.272   \n",
       "..                              ...                              ...   \n",
       "103                           2.105                            3.267   \n",
       "104                           2.327                            2.881   \n",
       "105                           2.370                            3.111   \n",
       "106                           2.115                            3.853   \n",
       "107                           2.419                            3.679   \n",
       "\n",
       "     rh_transversetemporal_part1_thickness  rh_insula_part1_thickness  \\\n",
       "0                                    2.329                      3.620   \n",
       "1                                    2.034                      3.588   \n",
       "2                                    2.337                      3.657   \n",
       "3                                    2.275                      3.121   \n",
       "4                                    2.445                      3.171   \n",
       "..                                     ...                        ...   \n",
       "103                                  2.257                      3.231   \n",
       "104                                  2.124                      3.159   \n",
       "105                                  2.190                      3.480   \n",
       "106                                  2.231                      3.187   \n",
       "107                                  1.970                      3.192   \n",
       "\n",
       "     rh_insula_part2_thickness  rh_insula_part3_thickness  \\\n",
       "0                        2.776                      3.282   \n",
       "1                        2.654                      3.124   \n",
       "2                        2.495                      2.669   \n",
       "3                        2.333                      2.604   \n",
       "4                        2.216                      2.659   \n",
       "..                         ...                        ...   \n",
       "103                      2.574                      2.920   \n",
       "104                      2.450                      2.753   \n",
       "105                      2.294                      2.571   \n",
       "106                      2.510                      2.759   \n",
       "107                      2.551                      2.855   \n",
       "\n",
       "     rh_insula_part4_thickness  \n",
       "0                        3.347  \n",
       "1                        3.214  \n",
       "2                        2.886  \n",
       "3                        2.731  \n",
       "4                        2.657  \n",
       "..                         ...  \n",
       "103                      2.899  \n",
       "104                      2.791  \n",
       "105                      2.875  \n",
       "106                      2.838  \n",
       "107                      2.985  \n",
       "\n",
       "[108 rows x 309 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CT_Dublin_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402f2a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 309)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get shape of df_adj\n",
    "\n",
    "CT_Dublin_adj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e598579",
   "metadata": {},
   "source": [
    "To get an idea of how many of the participants belong to control and patient group and to plot that we can simply execute the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e4153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    80\n",
       "1    28\n",
       "Name: Group, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CT_Dublin_adj['Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aeeb7ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Group', ylabel='count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='Group', data = CT_Dublin_adj, palette='hls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b23c0",
   "metadata": {},
   "source": [
    "The plot shows a clear unequal distribution of the participants in the Group variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3996d6",
   "metadata": {},
   "source": [
    "Because the LogisticRegression function from sklearn requires the inputs to be numpy arrays, in the following step the dataframe is converted to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c20c482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 2.18 , 2.382, ..., 2.776, 3.282, 3.347],\n",
       "       [0.   , 2.394, 1.973, ..., 2.654, 3.124, 3.214],\n",
       "       [0.   , 2.551, 2.567, ..., 2.495, 2.669, 2.886],\n",
       "       ...,\n",
       "       [1.   , 2.273, 2.559, ..., 2.294, 2.571, 2.875],\n",
       "       [1.   , 1.94 , 2.438, ..., 2.51 , 2.759, 2.838],\n",
       "       [1.   , 2.108, 2.269, ..., 2.551, 2.855, 2.985]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe as numpy array \n",
    "\n",
    "CT_Dublin_adj.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998c14a",
   "metadata": {},
   "source": [
    "### 2.2 Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7df3f",
   "metadata": {},
   "source": [
    "In the next steps, the **logistic regression** model is built. Firstly, the input and output should be defined. Our input contains the **CT** for all of the 308 brain regions, meaning that there are n=308 features in total. The output is within the Group variable containing label information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cd70850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input\n",
    "\n",
    "X = CT_Dublin_adj.iloc[:,1:308].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a858fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.18 , 2.382, 2.346, ..., 3.62 , 2.776, 3.282],\n",
       "       [2.394, 1.973, 2.534, ..., 3.588, 2.654, 3.124],\n",
       "       [2.551, 2.567, 1.954, ..., 3.657, 2.495, 2.669],\n",
       "       ...,\n",
       "       [2.273, 2.559, 2.578, ..., 3.48 , 2.294, 2.571],\n",
       "       [1.94 , 2.438, 2.272, ..., 3.187, 2.51 , 2.759],\n",
       "       [2.108, 2.269, 2.145, ..., 3.192, 2.551, 2.855]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d44d0e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 307)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa56b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "\n",
    "y = CT_Dublin_adj.iloc[:,[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73f341d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0d23e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5377250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6401e42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce2797",
   "metadata": {},
   "source": [
    "Now having defined our input and ouput data, to build the logistic regression model we need to split our data into train and test sets. For this, we use the train_test_split splitter function from Sklearn. The training set is the dataset on which the model is trained. This data is seen and learned by the model. The test set is a a subset of the training set and utilized for an accurate evaluation of a final model fit.\n",
    "With that function, the data gets divided into X_train, X_test, y_train and y_test. X_train and y_train are used for training and fitting the model. The X_test and y_test sets, however, are used for training the model if the correct labels were predicted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75fed4",
   "metadata": {},
   "source": [
    "But before splitting the data into training and testing set, we use the StandardScaler() function to standardize our data. The function standardizes every feature (each column) indivudally by substracting the mean and then scaling to unit variance (dividing all the values by the standard deviation). As a result, we get a distribution with a mean equal to 0 and with a standard deviation equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27639fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data and plit into training and testing set\n",
    "\n",
    "X_sc = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f821651",
   "metadata": {},
   "source": [
    "The test size indicates the size of the test subset, a random sampling without replacement about 75% of the rows , the remaining 25% is put into the test set. The random_state parameter allows you to reproduce the same train test split each time when the code is run. With a different value for random_state, different information would flow into the train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb98f736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state = 0, solver ='liblinear')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dea38f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ce39e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487eed3",
   "metadata": {},
   "source": [
    "### 2.3 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c1784",
   "metadata": {},
   "source": [
    "In the next step, the model is evaluated. To evaluate the model, a look at the **confusion matrix** is helpful. The **confusion matrix** provides information on the quality of the logistic regression model since it shows the predicted values from the model compared to the actual values from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58f882a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[ 9 10]\n",
      " [ 1  7]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "  \n",
    "print (\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1eeac9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 257.44, 'Predicted label')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the confusion matrix visually more appealing\n",
    "\n",
    "class_names=[0,1]\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "tick_marks = np.arange(len(class_names)) \n",
    "plt.xticks(tick_marks, class_names) \n",
    "plt.yticks(tick_marks, class_names) \n",
    "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g') \n",
    "ax.xaxis.set_label_position(\"top\") \n",
    "plt.tight_layout() \n",
    "plt.title('Confusion matrix', y=1.1) \n",
    "plt.ylabel('Actual label') \n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02311028",
   "metadata": {},
   "source": [
    "As the plot shows, there are in total 27 predictions (N=108 participants * 25% test size). The upper left square in the **confusion matrix** contains all the **true positive** cases meaning that when a participant predicted as control belongs to the control group. Accordingly, the bottom right square carries information on **true negative** cases meaning a participant predicted as patient was a patient. On the contrary, the upper right square contains **false positives** cases (participant predicted as patient but was control) and the bottom left square contains information on **false negative** cases (participant predicted as control but was patient). At first sight, the model seems to be good. However, there are other measures indicating the qualtiy of the model such as **accuracy, precision and recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2eeec99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5925925925925926\n",
      "Precision: 0.4117647058823529\n",
      "Recall: 0.875\n"
     ]
    }
   ],
   "source": [
    "#compute accuracy, precision, recall\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) \n",
    "\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred)) \n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e17262e",
   "metadata": {},
   "source": [
    "The accuracy measure indicates the percentage of correct predictions, in this case 74.07%. The precision measure shows the correct positive predictions relative to total positive predictions, here 0.667. The recall measure indicates the correct positive predictions relative to total actual positives, here 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031897f",
   "metadata": {},
   "source": [
    "Another measure that provides information on the diagnostic ability of a binary classifier is a Receiver Operator Characteristic (ROC) curve. A ROC curve plots the true positiv rate (proportion of observations that were correctly predicted to be positive out of all positive observations) on the y-axis against the false positive rate (proportion of observations that are incorrectly predicted to be positiv out of all negative observations)on the x-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4ba0b0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, classifier.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da090e03",
   "metadata": {},
   "source": [
    "Curves closer to the top-left corner indicate a better performance of the classifier. The closer the curve comes to the 45-degree diagonal of the ROC space, the less acurate the classifier is. As the plot shows, our model does not perform very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee9a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201bc680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6789515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567aea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600942b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3ab202",
   "metadata": {},
   "source": [
    "## Micro-structural data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba209f",
   "metadata": {},
   "source": [
    "### Mean diffusivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7217d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "\n",
    "MD_Dublin_path = os.path.join(os.pardir, 'data', 'PARC_500.aparc_MD_cortexAv_mean_Dublin.csv')\n",
    "MD_Dublin = pd.read_csv(MD_Dublin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust dataframe\n",
    "\n",
    "MD_Dublin_adj = MD_Dublin.drop(['Subject ID','Age', 'Sex'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label group 1 as 0 and 2 as 1\n",
    "\n",
    "MD_Dublin_adj['Group'] = MD_Dublin_adj['Group'].replace([1,2],[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_Dublin_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa974bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe as numpy array \n",
    "\n",
    "MD_Dublin_adj.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0decd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input\n",
    "\n",
    "X_MD = MD_Dublin_adj.iloc[:,1:308].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "\n",
    "y_MD = MD_Dublin_adj.iloc[:,[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7089e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a491660",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32979ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "\n",
    "X_train_MD, X_test_MD, y_train_MD, y_test_MD = train_test_split(X_MD, y_MD, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfae0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "xtrain_MD = sc_x.fit_transform(X_train_MD) \n",
    "xtest_MD = sc_x.transform(X_test_MD)\n",
    "  \n",
    "print(xtrain_MD)\n",
    "print(xtest_MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0679ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_MD = LogisticRegression(random_state = 0, solver ='liblinear')\n",
    "classifier_MD.fit(X_train_MD, y_train_MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MD=classifier_MD.predict(X_test_MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edc4b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred_MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "cm_MD = confusion_matrix(y_test_MD, y_pred_MD)\n",
    "  \n",
    "print (\"Confusion Matrix : \\n\", cm_MD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix visually more appealing\n",
    "\n",
    "class_names=[0,1]\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "tick_marks = np.arange(len(class_names)) \n",
    "plt.xticks(tick_marks, class_names) \n",
    "plt.yticks(tick_marks, class_names) \n",
    "sns.heatmap(pd.DataFrame(cm_MD), annot=True, cmap=\"YlGnBu\" ,fmt='g') \n",
    "ax.xaxis.set_label_position(\"top\") \n",
    "plt.tight_layout() \n",
    "plt.title('Confusion matrix', y=1.1) \n",
    "plt.ylabel('Actual label') \n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test_MD, classifier_MD.predict(X_test_MD))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_MD, classifier_MD.predict_proba(X_test_MD)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6bc86",
   "metadata": {},
   "source": [
    "## Fractional anisotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "\n",
    "FA_Dublin_path = os.path.join(os.pardir, 'data', 'PARC_500.aparc_FA_cortexAv_mean_Dublin.csv')\n",
    "FA_Dublin = pd.read_csv(FA_Dublin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust dataframe\n",
    "\n",
    "FA_Dublin_adj = FA_Dublin.drop(['Subject ID','Age', 'Sex'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2354f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label group 1 as 0 and 2 as 1\n",
    "\n",
    "FA_Dublin_adj['Group'] = FA_Dublin_adj['Group'].replace([1,2],[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe as numpy array \n",
    "\n",
    "FA_Dublin_adj.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa88998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input\n",
    "\n",
    "X_FA = FA_Dublin_adj.iloc[:,1:308].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6fdc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "\n",
    "y_FA = FA_Dublin_adj.iloc[:,[0]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bcb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "\n",
    "X_train_FA, X_test_FA, y_train_FA, y_test_FA = train_test_split(X_FA, y_FA, test_size = 0.25, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90885b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "xtrain_FA = sc_x.fit_transform(X_train_FA) \n",
    "xtest_FA = sc_x.transform(X_test_FA)\n",
    "  \n",
    "print(xtrain_FA)\n",
    "print(xtest_FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebff525",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_FA = LogisticRegression(random_state = 0, solver ='liblinear')\n",
    "classifier_FA.fit(X_train_FA, y_train_FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_FA =classifier_FA.predict(X_test_FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "cm_FA = confusion_matrix(y_test_FA, y_pred_FA)\n",
    "  \n",
    "print (\"Confusion Matrix : \\n\", cm_FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a037656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix visually more appealing\n",
    "\n",
    "class_names=[0,1]\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "tick_marks = np.arange(len(class_names)) \n",
    "plt.xticks(tick_marks, class_names) \n",
    "plt.yticks(tick_marks, class_names) \n",
    "sns.heatmap(pd.DataFrame(cm_FA), annot=True, cmap=\"YlGnBu\" ,fmt='g') \n",
    "ax.xaxis.set_label_position(\"top\") \n",
    "plt.tight_layout() \n",
    "plt.title('Confusion matrix', y=1.1) \n",
    "plt.ylabel('Actual label') \n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test_FA, classifier_FA.predict(X_test_FA))\n",
    "fpr, tpr, thresholds = roc_curve(y_test_FA, classifier_FA.predict_proba(X_test_FA)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79136692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "578504114d49301275c44c87035f08411733f9928d9347745d7de100c09f7611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
